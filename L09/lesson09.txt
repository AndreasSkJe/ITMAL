ITMAL NOTE til Slides:
LESSON 9: Optimering og Søgning

NOTE: lektion burde egentlig hedde "Regularisering og Søgning" eller evt.
"Regularisering, optimering og søgning", idet vi snakker mere om regularisering
og kun lidt om optimering.

-----------------------------------------------------------------------------

Slide 3 + 4:

Hvordan vælger vi i første omgang en ML algoritme (algo.  selection), dvs. 
skal vi bruge simpel lineær regression, et neuralt netværk (NN) eller en anden
model (algoritme) fra Scikit-learn?

Når en specifik algoritme er valg, hvordan tuner vi den så ind så den passer
til vore problem (model selection), dvs. hvis vi valgte en NN, hvor mangle lag
skal det så have, og hvor mange neuroner i hvert lag?

Når vi så har valgt algo og opsætning af vores model, så kan vi evaluere
modellen (model evaluation) med vores data (f.eks.  train/validate/test split),
dvs.  finde dens score, og gentage processen, hvis vi ikke er tilfreds med
scoren.  Processen kan re-itereres ved at gå tilbage til "model selection"
eller evt.  helt tilbage til "algorithm selection".

Slide 3: Bemærk at "ML algo + model seletion" dragen i "The Map": det er en meget
kritisk fase i ethvert ML projekt.

Slide 3: Model evaluation metoder i disse slides er taget fra artiklen til højre.

Slide 4: tekst beskrivelse af ML algo + model selection, med kritiske koncepter
i rød tekst.

Nye koncepter i slide 4 er:

* Hyperparametre: model parametre, der ikke direkte indgår i træningen af
  modellen.  Disse parameter ser man typisk i modellens constructor med default
  parametre ala "alpha=0.0001" osv. Hyperparametre kan også tunes, men så
  bliver omfanget af parametre vi kan skrue på enormt, så derfor kommer vi til
  at se på søgning i disse parametre som en alternativ metode.

* Regulizers kan hjælpe modellen til ikke at overfitte, eller prøve at lære al
  data incl. støj.

* Optimizers kan hjælpe læringen med at kører hurtigere, f.eks. kan man gøre
  Gradient Decent hurtigere ved at tilføje noget der ligner fysisk momentum
  (dvs.  når først gradienten bevæger sig hurtig, tager det lang tid at stoppe
  den igen) og fysisk friktion.  F.eks.  gik læring af moon data fra L07
  (L07/keras_mlp_moon.ipynb) ret langsom med mindre man valgte en god optimzer
  (optimizer = Adam(lr = 0.1) og  optimizer = SGD(lr = 0.1)). Optimizeren er en
  hyperparametre (Adam/SGD/osv.), som så i sig selv også har hyperparametre
  (lr=0.1 osv.).

* k-Fold cross validation (CV) har vi snakket om, men three-way split er en
  udvidet form for model validering.

-----------------------------------------------------------------------------

Slide 5: Simple Holdout

Mht. til CV og three-way split og model evaluering, er der (naturligvis) flere
avancerede måder at evaluere på.

Vi anvender som basis ALTID simple holdout/train-test split, hvor data deles i
et træningssæt og et testsæt (1).

Model trænes (2) på træningssæt og slut evalueres (3) med testsæt, hvorefter
model kører på nye data (4). Dette er magen til "train" (blå), "test" (gul) og
"inference/run" (grøn) blokkene i "The Map" (slide 3).

-----------------------------------------------------------------------------

Slide 6: Three-way Holdout for Hyperparameter Tuing

En mere avanceret model evaluering består i at dele i train/validate/test, for
herefter at sætte flere (i slide 3) modeller op med forskellige hyperparametre,
og så træne de forskellige modeller med train data.

Bemærk at det er samme grund-model (ML algo), dog bare sat op med forskellige
hyperparametre. Det er ikke tre forskellige ML algoritmer vi sætter op
samtidigt...

Herefter valideres de forskellige modeller på validation data og man vælger den
model, der har den bedste score, og smider de andre modeller væk.

Så gentages Simple Holdout på denne bedste model, nu med træning+validation
data lagt sammen til et nyt større træningssæt (4), og modellen slut-evalueres med
test data sættet (5) og er klar til at køre på ny data (5).

-----------------------------------------------------------------------------

Slide 7: k-fold CV

Så kigger vi tilbage på k-fold CV, og husker at den fungerede ved at man deler
data i train og test sæt, og herefter deler træning i k "trænings
fold/validation fold" (A) og træner en model på hver af disse folds (B).

Modellerne bruges til at finde en performance (score), dvs. validate data og
model i (C), og slut-performance (score) tages som en gennemsnit af alle
k-folds.

NOTE: Jeg har dog et fundamentalt problem med at forstå, hvordan man kan sige
at performance er et gennemsnit af forskellig sub-valideringer, og træner man
ikke her k-modeller, hvorefter man smider k-1 model væk? Alternativ er at man
træner een model, men så ser modellen undervejs alle data og det må den ikke
iforb. med validering...dvs. at jeg i k-fold CV ser nogle detaljer, jeg pt.
ikke kan redegøre for, men måske kan i?

-----------------------------------------------------------------------------

Slide 8: Model eval and Sel.

Ok, så blander vi alle de tidligere metoder til en ny "combo" metode.  Del i
train/test (1), lav en mængde modeller med forskellige hyperparametre, og træn
vi k-fold CV (2).

Find den model med bedste score fra step (2), og træn igen (?) med alle
træningsdata (3).

Test med test data (4) og voila, du har den bedste model fra et sæt med
forskellige hyperparametre, fundet via k-fold CV bladet med three-way
holdout...

-----------------------------------------------------------------------------

Slide 9: Regularization

Så hopper vi til et nyt koncept: regularisering. I sidste lektion snakkede vi
om generaliseringsfejlen, og om afstanden mellem trænings og test fejl i f.eks.
et Error-Capacity plot.

Generaliseringsfejlen er kunne falde indtil en optimal kapacitet, hvorefter den
kunne stige igen, og vi kaldte regionen til venstre i dette plot for
underfitting-zone og regionen til højre for den optimale kapacitet for
overfitting-zone.

Det kan være vanskeligt at bestemme den "optimale kapacitet" for en model, så
vi kunne i stedet bruge regulariserings-teknikker for at undgå at en model (med
for høj kapacitet) kommer til at overfitte!

Det gøres simpelt ved at lægge en penalty faktor, \Omega, til vores cost
funktion J, således at \Omega lægger en constraint på de interne vægte i
modellen, w (hvis vi antager at det er et NN), så de ikke kan vokse uhæmmet.

-----------------------------------------------------------------------------

Slide 10: L2 Regularization

Lad os prøve en simpel L2 \Omega, dvs. en penalty faktor baseret på vores
kendte L2-norm.

Denne kaldes L2-regulizer, Ridge penalization, weight decay, eller Tikhonov
regularization...mange forskellige navne for det SAMME koncept, så lad os holde
os til R2-regularisering/Ridge penalization her.

Den er simpel at skrive op med lineær algebra, vi bruger bare L2-normen og kan
finde \Omega som w^Transposed dot w.

J^~ bliver så bare J + \Omega ... og man kan se at \Omega bliver større jo
større de enkelte weight-komponenter er (w_1, w_2, osv.)

Bemærk også at w_0 (bias elementet) ikke deltager i \Omega udregningen.

-----------------------------------------------------------------------------

Slide 11: L2 Regularization

OK, hvordan fungerer L2-regulariseringen så på f.eks. en lineær regression. Det
optimale minimum i w-space (w1-w2 vs J, til venstre) er givet ved w*, dvs.
punktet i w1-w2 hvor J er minimal.

I 1D feature space kunne den optimale w, dvs. w*, være givet ved et fin fit af
x1 som linien, hvor w^* går 'pænt' igennem alle punkter. w' kunne være et
dårligt start fit, mens w'' viser, hvordan vi iterere ned mod w*.

Men det var for J, uden \Omega lagt til J, selve \Omega vil have et minimum i J
i origo, og hvis w* lå lidt væk for origo, vil de to led begynde at kæmpe om at
få "ret": the tug-of-war mellem disse to led gør at vi finder et nyt minimum,
på figuren til højre kaldet w~.

-----------------------------------------------------------------------------

Slide 12: L1 Regularization

Fra lineær algebra og afstands-mål kunne vi også have valgt L1-normen i stedet
for L2-normen. Denne L1-regulizer er også kendt under navnet Lasso-penalizer.

Så bliver \Omega blot sum af alle w led, dvs. \Omega = abs(w_1) + abs(w_2) + ..

L1-regularisering har nogle fordele ifht. L2-regularizering, vigtigst er dens
evne til at drive w-komponenter (f.eks. w_1 på figuren i slides) mod 0. Dette
sker pga. cost-funktionens "kantede" form, i stedet for L2-cost funktionens
bløde form, hvor L2-regularisering forsøger at drive w'erne mod origo.

L1-regularisering kan derfor drive en enkelt vægt (f.eks. w_1) mod nul, og hvis
den er nul kan man derfor sige at L1-regularisering kan lave automatisk feature
selektion, idet vi smider features for w_1=0 væk.

-----------------------------------------------------------------------------

Slide 13: L1 and L2 Regularization

Bemærk dog at L2-regularisering også har nyttige egenskaber ifhd.
L1-regularisering, så vi laver en hybrid mellem de to, dvs. at \Omega bliver en
lineær blanding af L1- og L2-regularisering...også kaldet Elastic-net
regularisering.

\beta (hyper)-parameteren bestemmer 'bladningsforholdet' ml. L1- og
L2-regularisering.

Og hvordan finder vi så den 'rigtige' værdi af \beta...ved søgning (som kommer
senere)!

I opgaven 'regulizers.ipynb' skal i bla. implementere en L2-regulizers \Omega
funktion, samt beskrive L1-, L2- og Elastic-net regulizers.

-----------------------------------------------------------------------------

Slide 14: Optimizers

Til gradient descent (GD) findes der en hel del optimiseringsteknikker, der kan
få læring til at gå hurtigere

Hvis startpunkt for en GD algo befinder sig oppe på et stort plateau (i J), vil
det kunne tage lang tid for den via den minimale gradient 'heroppe' begynder at
falde ned mod et minimum.

I moon data opgaven fra en tidligere lektion (07/keras_mlp_moon.ipynb) kunne
man se at der i lang tid ikke skete noget nævneværdigt med J, indtil et punkt
hvor den så hurtigt fald ned i et minimum. Valg af optimizer til denne opgave
kunne speede træningen gevaldigt op, mens forkert valg af
optimizer/hyperparametre for optimizer kunne gøre at træningens-J'en ikke
rykkede sig ud af stedet overhovedet!

Sliden viser en metode til at tilføje noget, der ligner fysisk momentum, til en
GD algoritme, dvs.  at når først at 'sne-bolden' ruller på figuren, så bliver
den større og større, idet den opsamler masse i form af 'sne' og den kan derfor
med nok fart og momentum kommer ud af et mindre minimum, w', for endeligt at
rulle ned et dybere minimum w*.

Uden momentum ville vi finde w'.

Herudover er der masser af læsemateriale om optimizers i [HOLM] og på
Scikit-learn, men der er ingen direkte opgave i optimizers.

-----------------------------------------------------------------------------

Slide 15: ML Models

Hvilken model skal vi vælge? Well, hvilke modeller har vi kigget på indtil nu:
KNNs, Lineære modeller, SGD modeller og lidt MLP NN's (uden at gå i dybden).

Men der i Scikit-learn en stor vifte af helt forskellige ML modeller (algos),
f.eks. var Support Vector Machines (SVM) de helt varme modeller ;-), for en
dekade siden...nu igen overtaget af NN's.

Interessant er mængden af eksotiske modeller ;-) f.eks. Bagging, RandomForest
osv. og går man lidt på opdagelse kan man finde modeller, der båder er meget
hurtige at træne, men også performer ligeså godt, nogle gange bedre, end de
populære NN's.

-----------------------------------------------------------------------------

Slide 16: Model Selection via Searching

Derfor varmer vi også op til at i skal lave en søge-opgave, hvor i løser MNIST
via diverse modeller, bare IKKE NN's...

Man kunne vælge model ved struktureret at læse dokumentation igennem, undersøge
model skalerbarhed/complexity og sammenholde med viden om ens datasæt. Dette er
tungt og langsomt.

Alternativt kunne man bare sætte computeren til at søge i f.eks. forskellige
ML modeller, for at se, hvilken model der er bedst til de nuværende data.

Det er nemt i python, lav et sæt af ML modeller (linie 1-5), loop over dette sæt
(7), og kør en fit-predict for at finde scores (8-11).  Vælg da den model med
bedste score, for dette model- og datasæt blev det GaussianNB (NB=Naive Bayes)
med en score på 1!.

Gaussian modeller er kendt for at være hurtige at træne, men de kan mangle lidt
'kapacitet' når data bliver for store/komplekse.


-----------------------------------------------------------------------------

Slide 17: Model Selection via Grid Search

Det var en metode til at søg i flere forskellige modeller, hvis du vælger en
enkelt model, hvordan vil du så sætte modellens hyperparametre.

Koden viser hyperparametre for en SGDRegressor, og der er ca. 20 forskellige
parametre at sætte. Vælg som udgangspunkt den defaulte værdi, f.eks. 0.0001 for
alpha.

Men skal du finde en optimal model bliver kan du søge i et (mindre) sæt af
interessante hyperparametre. Den nederste kode viser, hvordan søge parametre
sættes op for SGD'en, her er alpha, max_iter og learning_rate udvalg, og
søgeparametrene sættes op i linie 2-6.

Køre herefter "GridSearchCV", med SGD modellen (model), og søgeparametersættet
(tuning_parameters), samt evt. nogle ekstra parameter til GridSeachCV
funktionen.


-----------------------------------------------------------------------------

Slide 18: Model Selection via Grid Search

Brute-force søgningen via GridSearchCV går stille og roligt ALLE kombination af
alle søgeparameter igennem, dvs. laver et fint plot ala det viste (for
kombinationen af alpha/max-iter) og finder den bedste score (ud fra en CV
metode).

Den de bedste hyperparametre er således den kombination med højeste score
(internt bruger ML jo J, men vi som mennesker er jo interesseret i score'en ikke
loss-funktionen).

Kombinatorikken gøre at med blot en mindre sæt hyperparametre, så kan det tage
lang tid for grid searchen af blive færdig.

Og desværre skriver Scikit-learns grid-search ikke noget ud om hvor langt den er
kommet, eller om det vil tage timer eller måneder før den er færdig. Derfor vil
i opleve at man sætter en grid search i gang, venter noget tid, stopper den
inden den er færdig, reducerer i søgeparameter, sætter den i gang igen, og når
så igennem denne uproduktive iteration flere gange uden at få et reelt output!


-----------------------------------------------------------------------------

Slide 19: Model Selection via Randomized Search

Så i stedet for slavisk at gå igennem alle kombinationer, kunne man blot iterere
tilfældige kombinationer, så score-'overfladen' hurtigere bliver afsøgt for evt.
maksima.

Random search søger således samme søgerum som grid-search, men blot i en anden
rækkefølge, og vi sætter så blot random search til at udtrække n-kombinationer
hvor grid search søger i samtlige kombinationer.

Derfor kommer random search ud med NOGET efter n-kombinationer, klart at det
sikkert ikke er det samme maksimum som grid-search, men gør det noget?

I opgaven 'gridsearch.ipyb' skal i selv ind og foretage grid- og
random-søgninger, for til sidst at søge efter den bedste model+hyperparams for
at løse MNIST opgaven (dog uden brug af NN modeller)...

-----------------------------------------------------------------------------

END