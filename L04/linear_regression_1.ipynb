{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS|\n",
    "---------|------------------------------------------------\n",
    "2018-1218|CEF, initial.                  \n",
    "\n",
    "## Linear Regression I\n",
    "\n",
    "The goal of the linear regression is to find the argument $w$ that minimizes the sum-of-squares error over all inputs. \n",
    "\n",
    "Given the usual ML input data matrix $\\mathbf X$ of size $(n,d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\newcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\newcommand\\dfrac[2]{\\frac{\\mbox{d}~#1}{\\mbox{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "and $\\by$ is the target output column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\by =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1} \\\\\n",
    "     y\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     y\\pown{n} \\\\\n",
    "  }\n",
    "$$\n",
    "\n",
    "The linear regression model, via its hypothesis function and for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$ (with the additional element $w_0$ being the bias), can now be written as simple as\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\ac{c}{1\\\\\\bx\\powni} \\\\\n",
    "                   &= w_0 + w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni\n",
    "}\n",
    "$$\n",
    "\n",
    "using the model parameters or weights, $\\bw$, aka $\\btheta$. To ease notation $\\bx$ is assumed to have the 1 element preppended in the following, so that $\\bx$ is a $d+1$ column vector\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni, ~~~~\\mbox{by convention in the following...}\\\\\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\bx\\powni \n",
    "}\n",
    "$$\n",
    "\n",
    "This is actually the first fully white-box machine learning algorithm, that we see. All the glory details of the algorithm is clearly visible in the internal vector multiplication...quite simple, right? Now we just need to train the weights...\n",
    " \n",
    "### Loss or Objective Function\n",
    "\n",
    "The individual cost (or loss), $L\\powni$, for a single input-vector $\\bx\\powni$ is a measure of how the model is able to fit the data: the higher the $L\\powni$ value the worse it is able to fit. A loss of $L=0$ means a perfect fit.\n",
    "\n",
    "It can be given by, say, the square difference form the calculated output, $h$, to the desired output, $y$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= \\left( h(\\bx\\powni;\\bw) - y\\powni \\right)^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2 \n",
    "}\n",
    "$$\n",
    "\n",
    "To minimize all the $L\\powni$ losses (or indirecly also the MSE or RMSE), is to minimize the sum of all the\n",
    "individual costs, via the total cost function $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "\\mbox{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                    &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2\\\\\n",
    "                    &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expression.\n",
    "\n",
    "Now the factor $\\frac{1}{n}$ is just a constant, and can be ignored, yielding the total cost function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    J &= \\frac{1}{2} ||\\bX \\bw - \\by||_2^2\\\\\n",
    "     &\\propto MSE\n",
    "}\n",
    "$$\n",
    "\n",
    "adding yet another constant, 1/2, to ease later differentiation of $J$.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training the linear regression modl now amounts to computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimzes the total cost\n",
    "\n",
    "$$\n",
    " \\bw^* = \\mbox{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "where $\\mbox{argmin}_\\bw$ means find the argument of $\\bw$ that minimizes the $J$ function. This minima (sometimes a maxima) is denoted $\\bw^*$ in most ML literature. \n",
    "\n",
    "### Training: The Closed-form Solution\n",
    "\n",
    "To solve for $\\bw^*$ in closed form (i.e. directly, without any numerical approximation), we find the gradient of $J$ with respect to $\\bw$. Taking the partial deriverty $\\partial/\\partial_\\bw$ of the $J$ via the gradient (napla) operator\n",
    "\n",
    "$$\n",
    "\\rem{\n",
    " \\frac{\\partial}{\\partial \\bw} = \n",
    "   \\ac{c}{\n",
    "     \\frac{\\partial}{\\partial w_1} \\\\\n",
    "     \\frac{\\partial}{\\partial w_2} \\\\\n",
    "     \\vdots\\\\\n",
    "     \\frac{\\partial}{\\partial w_d}\n",
    "   }\n",
    "}    \n",
    " \\nabla_\\bw~J = \n",
    "   \\left[ \\frac{\\partial J}{\\partial w_1}, \\frac{\\partial J}{\\partial w_2}, \\ldots ,  \\frac{\\partial J}{\\partial w_m}   \\right]^\\top\n",
    "$$\n",
    "     \n",
    "and setting it to zero yields the optimal solution for $\\bw$, and ignoring all constant factors of 1/2\n",
    "and $1/n$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\by \\right) ~=~ 0\\\\\n",
    "                  0 &=~ \\bX \\bw - \\by\n",
    "}\n",
    "$$\n",
    "\n",
    "give the closed-form solution, with $\\by = [y\\pown{1}, y\\pown{2}, \\cdots,\n",
    "y\\pown{n}]^\\top$\n",
    "\n",
    "$$\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "You already now this method from math, finding the extrema for a function, say \n",
    "\n",
    "$$f(w)=w^2-2w-2$$ \n",
    "\n",
    "so is given by finding the place where the gradient $\\mbox{d}~f(w)/\\mbox{d}w = 0$\n",
    "\n",
    "$$\n",
    "   \\dfrac{f(w)}{w} = 2w -2 = 0\n",
    "$$\n",
    "\n",
    "so we see that there is an extremum at $w=1$. Cheking the second deriverty tells if we are seeing a minimum or maximum at that point, in matrix terms this corresponds to finding the _Hessian_ matrix.\n",
    "\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Hessian_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qa Find the closed-form optimal value for $\\bw$ for the linear system\n",
    "$$\n",
    "    f(x) = w_0 + w_1 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa (no python code)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q b Write a Python function that uses the closed-form solution to find the argmin for $\\bw$\n",
    "\n",
    "Also create suatable test data and test vectors (test functions) for the implementation. \n",
    "\n",
    "Share your test vectors on the excercise discussion board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
