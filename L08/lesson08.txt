ITMAL
NOTE til Slides: LESSON 8: Model-capacity, Under- and Overfitting, Generalization

-----------------------------------------------------------------------------

Slide 1 til 6: 

Real-world systemer, der anvender ML. 

(skulle have været en klasse diskussion).

-----------------------------------------------------------------------------

Slide 1: Shimon

En ML-robot, der spiller musik. Var forbi Århusmusikhus i efteråret. 

'Kreavtiv' robot idet fejl (FP og FN), leder til alternative fortolkniner af
musikken...den spiller jo Jazz! Herved giver False-Positive og False-Negative
direkte værdi istedet for at give problemer!

Robotten ledsages af omkring 5-6 ingeniører og en professor, så den tager ikke
arbejdet fra os!

-----------------------------------------------------------------------------

Slide 2: Busser

Nyligt indsatte busser i Aalborg Øst, dansk firma, teknologi ikke kendt og er
ikke at finde på hjemmesiden. 

Kører 18 til 25 km/t og der sidder _stadig_ en chauffør i bussen med et
joystick og nødstop.

Hvornår køerer den 80 km/t og uden chauffør?

-----------------------------------------------------------------------------

Slide 3: På roadtrip med en insekthjerne

Brug af ML til at lave beat-poesi, igen giver FP og FN direkte (positiv) værdi,
som kreativt indspark fra en "insekthjerne" (ML system).

Når ML er nået til Weekendavisen (kræver helst at læseren ryger pipe eller er over
45 år, helst begge dele), er vi så ikke nået toppen af "Expectation-Time"
kurven, og står over for at ryge ned i "Trought of Disillusionment"?

Se også referencen til "AlphaGo", som vi snakkede om i lektion 1.
 
-----------------------------------------------------------------------------

Slide 4+5: Detektering af mide på bier

System lavet her på ASE til at finde mider på bier. Se en mide i den røde
firkant på Slide 4 (den er ikke ret tydelig).

Brug af ML indgår som et neuralt-netværk (CNN), og er kun en meget lille del af
den totale billeprocesserings-pipleline (Se pipeline, Slide 5, se CNN i hjørnet
af "Varroa classification and localization" block).

Dvs. at dette system ikke direkte kunne bruge ML til at finde mider, men kun
benytte ML (CNN) som en lille sub-system til at løse hele problem-komplekset.

Krævede mange ingeniører og en del udviklings-år til konstruktion..igen nemme
ML løsninger direkte, øv!

-----------------------------------------------------------------------------

Slide 6: kløver mark med insekter

BA projekt fremlægges, generelt Tagging/labeling tool til at finde insekter!

Problemstilling: lav en system, der kan finde insekter i billeder ala denne
kløvermark. Billeddata er i TerraByte størrelsen! Bagrund og belysning
forskellig (nat/dag, regn/sol, skygge + vandreflektioner osv.).

For at kunne lave dette skal der laves et datagrundlag til Supervised
learning...og hermed et værktøj, så vi kan sidde og manuelt finde insekter.

BA projektet går ud på at lave et sådanne generelt labeling værktøj. Ellers
frie hænder mht. GUI (QT, Web) men systemet skal kunne kører på både Linux og
Windows...

-----------------------------------------------------------------------------

Slide 7: The Map

Igang med dagens emne: træning og generalizering.  Dvs.  at vi kigger på hvad
der sker når vi har trænet og går i gang med at se på nye data: ind kommer
X^{new} som så bliver outputtet som y^{new} i inference/run blokken (grøn
tekst). 

Dette viser, hvordan ML systemet er i stand til at generalizere (grøn kasse i
midten!).

Husk på at vores dummy binary classifier til MNIST havde en accuracy på ca 90%,
men ikke besad nogen form for generalizerings evne!  Dvs.  at den i
ML-sammenhæng er ligegyldig...et ML system skal kunne forudsige på nye data med
en hvis kvalitet.
 
-----------------------------------------------------------------------------

Slide 8: Pipelines

Kort intro til brug af pipelines i Python. Når i laver jeres eget system, vil i
skulle pre-processere jeres data igen-og-igen. 

I en pipeline loader man derfor de rå data, og introducere så flere og flere
blokke, der modificerer eller reducerer data inden selve træning går
igang...det minder lidt om min billedbehandlings-pipeline på Slide 5, for selve
"Low-level processing" pipelinen.

Det essentielle i Pipelinen er (lidt som i en testsuite) at man kan
automatisere processen, og med pipeline-koden har en "formel" beskrivelse af,
hvad man gør med data, istedet for at det bliver en manuel process, der er svær
eller umulig at genskabe. 

Pipelines i Python følger fit (predict/transform?) interfacet, og husk at nye
data skal preprocesseres på samme måde som jeres test data, dvs. kører igennem
samme pipeline som under træning.

-----------------------------------------------------------------------------

Slide 9: Resume, performance_metrics.ipynb

Vi hopper kort tilbage til loss funktioner og scores/performace metrikkerne,
som i nu har styr på. 

Kørte i Keras MLP opgaven fra sidste lektion, så i at loss funktionen
"categorical_crossentropy" blev brugt til træning, selvom at vi primært har
behandlet loss funktionerne MSE (evt. RMSE), og MAE. En meget populær loss
funktion er "crossentropy" (som også kaldes log loss/logistic). Den har vi ikke
behandlet men detaljer kan findes i Scikit-learn (hvis du har mod):

  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html?highlight=cross%20entropy

I Scikit-learn finder i også, som sædvanligt, en hel menu af loss-funktioner
(kaldet Regression metrics i slide), og grænsen mellem en loss- og en score-
funktion udviskes gradvist. Generelt er en høj værdi for en score funktion at
foretrække (jo højere score jo bedre), mens en lav loss er bedst (jo lavere
loss værdi jo bedre). Derfor kunne man f.eks. lave en score der var lig -MSE
(dvs. minus mean-square-error loss'en).

Tidligere brugte bla.  accuracy som score (der ikke er en rigtig
metrik/afstandsmål). Den hedder i Scikit-learn "metrics.accuracy_score"
og i Keras "categorical_accuracy". 

Bemærk her at Keras' dokumentation er meget mangelfuld i forhold til
Scikit-learn. Tilgengæld er perfomance af Keras (+Tensorflow) i top, mens
performance af Scikit-learn er i bund: god performance er ikke et design pricip
i Scikit-learn, god dokumentation er ikke et design pricip for Keras.

Endelig viste Keras MLP-opgaven at man kunne opsamle og gemme diverse metrikker
undervejs, her er f.ek.s "categorical_accuracy", "MSE" og "MAE" gemt i en
historik, som så kan plottes til sidst. Under træning kan der gå lang tid før
at MSE/MAE går ned eller at accuracy går op, fordi 'systemet' befinder sig på
et plateu, hvor det tager lang tid før det når ned i et minimum.

Ved tuning af model-hyperparametre, kan man få 'systemet' til hurtigere at
iterer sig ned mod et minimum...det kommer vi ind på under "Søgnings"-lektionen
næste gang.

-----------------------------------------------------------------------------

Slide 10: Model capacity

Et ML-system skulle kunne generalisere. Dummy binær classifieren havde en
"model kapacitet" på nul, og kunne ikke generalisere.

For et polynomisk fit kan man ca. sige at modelkapaciteten er "lig"  graden af
polynomiet, dvs. jo højere polinomiumsgrad jo større kapacitet.

For et NN, er det sværere direkte at sige noget om model kapaciteten, men den
skalerer på en måde med antallet af neuroner, jo flere neuroner, jo større
model kapacitet.

Nu kan man så skrue kapacitetet op så meget man vil og træningsfejlen vil gå
mod nul, men det gør så ikke noget godt for systemets evne til at generalisere,
for hvad sker der for et system med al for stor model-kapacitet, når vi kører
på nye data (X^{new} i The Map)?

Og hvordan vælger du den "optimale" model kapacitet?

-----------------------------------------------------------------------------

Slide 11: Under- and overfitting

Opgave til aflevering: "capcity_under_overfitting.ipynb", som viser de to
koncepter: under- og overfitting af data. 

Vi har snakket kort om under- og overfitting, men opgaven her går ud på at se
helt ned i detaljerne. En Cosinus funktion med støj forsøges at blive fittet
med en Polynomiums-model af degree=1, 4 og 15. 

De forskellige degree's leder så til hhv. under og overfitting, fordi
model-kapaciteten er hhv. for lille og for stor.

Men hvordan vælges så den optimale model kapacitet...det mangler vi stadig at
kunne svare på!?

-----------------------------------------------------------------------------

Slide 12: Generalizerings Error

Opgave til aflevering: "generalization_error.ipynb", som viser noget om alle
koncepter, model capacitet, under- og overfitting, og generaliseringsevne
samtidigt.

Svaret på hvordan man vælger den optimale model kapacitet ligger i
fortolkningen af Error-Capacity plots som set på denne slide.

Lader man model kapaciteten stige og plotter man så træningsfejen, kan man få
den til at gå mod nul, idet 'systemet' så bare lærer at følge data 100% incl.
støj og outliers. Dvs. at dets evne til at generalisere faktisk bliver mindre,
når model kapaciteten øges ud over den optimale kapacitet.

Generalizeringsfejlen, dvs. den fejl man begår på data som træningen ikke har
set (validation, test eller helt ny data), kan have en kurve, der har et
minimum ved en given kapacitet, dvs. den optimale model kapacitet---set ved den
røde vertikale linie i Error-Capacity plottet til højre. Denne optimale
kapacitet indeler også plottet i de to formelle zoner, under- og
overfitting zone.

i) RMSE-Training set size plottet til venstre viser en "learning curve" som
fundet i HOLM.

iii) RMSE-Epoch plottet viser et error-epoch som siger noget om
"early-stopping".

Men jeg finder plot error-capacity plottet ii) den bedste visualiseringsform til at
bestemme optimal capacity.

For vores dummy binary classifier (til MNIST) ville vi ikke kunne lave et
Error-Capacity plot, idet kapaciteten er froset fast til nul (capacity=0).
Herudover vil i) læringskurven være flad og iii) error-epoch plottet ogs vise
et system totalt uafhænging af antal epocher vi kører, den er ude af standt til
at lære/generalisere!

-----------------------------------------------------------------------------
END