{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2$^1$\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Vector and Matrix representation in Python\n",
    "\n",
    "Say, we have $d$ features for a given sample point. This $d$-sized feature column vector for a data-sample $i$ is then given by\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, rember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "$$    \n",
    " \n",
    "$$\n",
    "    \\bx^{(i)} = \n",
    "        \\left(\n",
    "            x_1\\powni ~ x_2\\powni ~ \\cdots ~  x_d\\powni\n",
    "         \\right)^T\n",
    "$$\n",
    "\n",
    "The full data matrix $\\bX$ is then constructed out of $n$ samples of these feature vectors\n",
    "\n",
    "$$\n",
    "    \\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots \\\\\n",
    "            x_1\\pownn & x_2\\pownn & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "such that $\\bX$ can be expressed as\n",
    "\n",
    "$$\n",
    "    \\bX = \n",
    "      \\ac{c}{\n",
    "        (\\bx\\pown{1})^T \\\\\n",
    "        (\\bx\\pown{2})^T \\\\\n",
    "        \\vdots \\\\\n",
    "        (\\bx\\pownn)^T\n",
    "      }\n",
    "$$\n",
    "\n",
    "but sometimes the notation is a litte more fuzzy, leaving out the transpose operator for $\\mathbf x$ and in doing so just interprenting the $\\mathbf{x}^{(i)}$'s to be row vectors instead of column vectors.\n",
    "\n",
    "The target column vector, $\\mathbf y$, has the dimension $n$ \n",
    "\n",
    "$$\n",
    "    \\by = \\ac{c}{\n",
    "            y\\pown{1} \\\\\n",
    "            y\\pown{2} \\\\\n",
    "            \\vdots \\\\\n",
    "            y\\pownn \\\\\n",
    "          }\n",
    "$$\n",
    "\n",
    "#### Qa Given the following $\\mathbf{x}^{(i)}$'s, construct and print the $\\mathbf X$ matrix in python.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "      \\bx\\pown{1} &= \\ac{c}{ 1, 2, 3} \\\\\n",
    "      \\bx\\pown{2} &= \\ac{c}{ 4, 2, 1} \\\\\n",
    "      \\bx\\pown{3} &= \\ac{c}{ 3, 8, 5} \\\\\n",
    "      \\bx\\pown{4} &= \\ac{c}{-9,-1, 0}\n",
    "    }\n",
    "$$\n",
    "\n",
    "### Norms or Distances\n",
    "\n",
    "The $L^2$ Euclidian distance, or norm (aka lenght), for a vector of size $n$ is defined as \n",
    "\n",
    "$$\n",
    "    L^2:~~ ||\\bx||_2 = \\left( \\sum_{i=1}^{n} |x_i|^2 \\right)^\\half\\\\\n",
    "$$\n",
    "\n",
    "and the distance between two vectors are given by\n",
    "\n",
    "$$\n",
    "    \\ar{ll}{      \n",
    "          d(\\bx,\\by) &= ||\\bx-\\by||_2\\\\\n",
    "                     &= \\left[ \\sum_{i=1}^n \\left( x_{i}-y_{i} \\right)^2 \\right]^\\half\n",
    "    }\n",
    "$$ \n",
    "\n",
    "This Euclidian norm is sometimes also just denoted as $||\\bx||$, leaving out the 2 in the subscript.\n",
    "\n",
    "The squared $L^2$ can be expressed via vector operations\n",
    "\n",
    "$$\n",
    "    ||\\bx||_2^2 = \\bx^\\top\\bx\n",
    "$$\n",
    "\n",
    "by the dot or innner-product\n",
    "\n",
    "$$\n",
    "    \\ar{ll}{\n",
    "        \\langle\\bx,\\by\\rangle &= \\bx\\cdot\\by\\\\ \n",
    "                              &=\\bx^\\top \\by\\\\\n",
    "                              &= \\sum_{i=1}^n x_{i} y_{i} \\\\\n",
    "                              &= ||\\bx|| ~ ||\\by|| \\cos{\\theta}\n",
    "    }\n",
    "$$\n",
    "\n",
    "taking $\\theta$ to be zero, and hence $cos(\\theta)=1$ when calculating $\\langle\\bx,\\bx\\rangle$\n",
    "\n",
    "The $L^1$ 'City-block' norm is given by\n",
    "\n",
    "$$\n",
    "    L^1:~~ ||\\bx||_1 = \\sum_i |x_i|\n",
    "$$\n",
    "\n",
    "but $L^1$ is not used as intensive as its more popular $L^2$ cousin.\n",
    "\n",
    "#### Qb Implement the $L^1$ and $L^2$ norms for vectors in Python.\n",
    "\n",
    "First, do not use any build-in methods, not even x.dot(). \n",
    "\n",
    "But test you implementation against some build in functions, say  ```numpy.linalg.norm```\n",
    "\n",
    "When this works, and passes your tests, optimize the $L^2$, such that it uses np.numpy's dot operator istead of an explicite sum. This implementation must be pythonic, i.e. it must not contain explicit for- or while-loops. \n",
    "\n",
    "## Performance Measures\n",
    "\n",
    "Now, most ML algorithm uses norms internally when doing searches. Details on this will come later, but for now we need to know that an algorithm typically tries to minimize a given performance measure for all the input data, and implicitly tries to minimize the sum of all norms for the 'distance' between some predicted output, $\\by_{pred}$ and the true output $\\by_{true}$, with the distance between these typically given by the $L^2$ norm\n",
    "\n",
    "$$\n",
    "    \\mbox{error}\\powni = d(\\by_{pred}\\powni,\\by_{true}\\powni) = ||\\by_{pred}\\powni-\\by_{true}\\powni||_2\n",
    "$$ \n",
    "\n",
    "and the total error will be the sum over all $i$'s, and this is so important in ML, that we will put it into the formal definition of a loss (or objective, or cost) function, so put on you best math-hat, now.. \n",
    "\n",
    "### Loss or Objective Function using the Mean Squared Error\n",
    "\n",
    "The performance metric is typically known as $J$ in ML and the _cost_ (or _loss_) function for a single input-point $\\bx\\powni$ can be given\n",
    "by the square difference form the calculated output, $h_\\bw$, to the desired\n",
    "output, $y$\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J\\powni &= \\left( h(\\bx\\powni) - y\\powni \\right)^2\n",
    "    }\n",
    "$$\n",
    "\n",
    "with $h$ being a prediction function, that maps the input vector $\\bx$ to a scalar\n",
    "\n",
    "$$\n",
    "    h(\\bx\\powni) = y_{pred}\n",
    "$$\n",
    "\n",
    "This is the internal function, that we train, to become better, and we will dig into the $h$ function once we reach deep learning.\n",
    "\n",
    "This was the scalar version, now for the full vector formulation: To minimize the MSE (or indirecly also RMSE), is to minimize the _sum of all the\n",
    "individual costs_, $J\\powni$\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\mbox{MSE}(\\bX,h) &= \\frac{1}{n} \\sum_{i=1}^{n} J\\powni(\\bw)\\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "Now the factor $\\frac{1}{n}$ is just a constant, and can be ignored, yielding\n",
    "the total cost function\n",
    "\n",
    "$$\n",
    "    J(\\bw) = ||h(\\bx\\powni) - \\by||_2^2\n",
    "$$\n",
    "\n",
    "This was the  $J$ formulation using the MSE, while other functions are possible.\n",
    "\n",
    "### RMSE\n",
    "\n",
    "Now, lets take a look on how you calculate the RMSE.\n",
    "\n",
    "The RMSE uses the $L^2$ norm internally, well, actually $||\\cdot||^2_2$ to be precise, and basically just sums, means and roots the squared error $\\mbox{error}\\powni$, we just saw before. \n",
    "\n",
    "A MSE is just a RMSE just without the final square-root call.\n",
    "\n",
    "\n",
    "### Qb Construct the Root Mean Square Error (RMSE) function (Equation 2-1 [HOLM]).\n",
    "\n",
    "Evaluate it for the $\\bX$-$\\by$'s, with $\\bX$ consisting of the four $\\bf\\powni$, $\\by$ given below, and taknig the prediction function $h(\\bx\\powni)$ to be a 'dummy' prediction in the form\n",
    "\n",
    "$$\n",
    "    h(\\bx\\powni) = x_1\n",
    "$$\n",
    "\n",
    "that is, just slice out the first $x$ scalar element.\n",
    "\n",
    "Test vector for RMSE and the $\\bX$ and $\\by$'s given below\n",
    "\n",
    "```python\n",
    "r=RMSE(X,y)\n",
    "expected=5.70087712549569\n",
    "print(\"RMSE=\",r,\", diff=\",r-expected)\n",
    "```\n",
    "\n",
    "### MAE\n",
    "\n",
    "#### Qc Similar construct the Mean Absolute Error (MAE) function (Equation 2-2 [HOLM]) and evaluate it.\n",
    "\n",
    "The MAE will algorithmic wise be similar to the MSE part from using the $L^1$ instead of the $L^2$ norm.\n",
    "\n",
    "Test vector for MAE still for the $\\bX$ you found and the $by$ given below\n",
    "\n",
    "```python\n",
    "r=MAE(X,y)\n",
    "expected=4.5\n",
    "print(\"MAE=\",r,\", diff=\",r-expected)\n",
    "```\n",
    "\n",
    "## Pythonic Code\n",
    "\n",
    "### Robustness of Code\n",
    "\n",
    "Data validity checking is an essential part of robust code, and in Python the 'fail-fast' method is used extensively: in stead of lingering on trying to get the 'best' out of an erroneous situation (like in Web), the fail-fast pragma will be very loud about any data inconsistencies at the earliest possible moment.\n",
    "\n",
    "Hence robust code should include a lot of error checking, say as pre- and post-conditions to calling a function. Normally assert-checking or exception throwing will do the trick just fine, with the exception method being more _pythonic_.\n",
    "\n",
    "For the norm-function you could, for instance, test your input data to be 'vector' like, i.e. like\n",
    "\n",
    "```python\n",
    "    assert x.shape[0]>=0 and x.shape[1]==0\n",
    "    \n",
    "    if not x.ndim==1:\n",
    "        raise some error\n",
    "```\n",
    "or similar.\n",
    "\n",
    "#### Qd Robust Code \n",
    "\n",
    "Add error checking code (asserts or exceptions) that checks for right $\\mathbf X$-$\\mathbf y$ sizes of the MAE, MEA and Cross entropy performance metrics.\n",
    "\n",
    "Also add error checking to all you previously tested $L^2$ and $L^1$ functions, and rerun your tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qa\n",
    "\n",
    "% reset -f\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([1,2,3,4]) # NOTE:  you'll need this later\n",
    "\n",
    "x1 = np.array([ 1, 2,3])\n",
    "x2 = np.array([ 4, 2,1])\n",
    "x3 = np.array([ 3, 8,5])\n",
    "x4 = np.array([-9,-1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL]\n",
    "NOTE: not finished\n",
    "\n",
    "### Cross entropy\n",
    "\n",
    "Entropy can be given by\n",
    "\n",
    "Information entropy, $H$ is the average rate at which information is produced by a stochastic source of data [https://en.wikipedia.org/wiki/Entropy_(information_theory)]\n",
    "\n",
    "$$\n",
    "    \\ar{H(X)}{\n",
    "      \\mathbb{E}[\\mathrm{I}(X)] \n",
    "        &= \\mathbb{E}[-\\ln(\\mathrm{P}(X))] \\\\\n",
    "        &= \\sum_{i=1}^n {\\mathrm{P}(x_i)\\,\\mathrm{I}(x_i)} \\\\\n",
    "        &= -\\sum_{i=1}^n {\\mathrm{P}(x_i) \\log_2 \\mathrm{P}(x_i)}\n",
    "} \n",
    "$$\n",
    "\n",
    "where $b$ is the base of the logarithm used. Well, thanks, \n",
    "\n",
    "#### Qd Again, construct the Cross Entropy Error function and evaluate it. \n",
    "\n",
    "TODO: add algo or python code for doing this!\n",
    "\n",
    "You find a definition of Cross entrypy here\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# from https://stackoverflow.com/questions/47377222/cross-entropy-function-python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n",
    "\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "targets = np.array([[0,0,0,1],\n",
    "                    [0,0,0,1]])\n",
    "\n",
    "ans = 0.71355817782  #Correct answer\n",
    "x = cross_entropy(predictions, targets)\n",
    "print(np.isclose(x,ans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
