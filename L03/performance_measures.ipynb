{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "# NB: øvelse ikke færdig pt, gå IKKE igang med den!\n",
    "\n",
    "REVISIONS|\n",
    "---------|------------------------------------------------\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0206| CEF, updated and spell checked. \n",
    "2018-0206| CEF, added Kaggle dataset exercise. \n",
    "\n",
    "## Performance Measures\n",
    "\n",
    "True positives, $TP$; false positives, $FP$; true negatives, $TN$ (type I\n",
    "error); and false negatives, $FN$ (type II error), and $N = N_P + N_N$ being\n",
    "the total number of samples and the number of positive and negative samples\n",
    "respectivly.\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$\n",
    "\\def\\ba{\\begin{array}{lll}}\n",
    "\\def\\ea{\\end{array}}\n",
    "\\newcommand{\\rem}[1]{}\n",
    "\\newcommand{\\subtext}[1]{_{\\scriptsize{\\mbox{#1}}}}\n",
    "\\newcommand{\\st}[1]{\\subtext{#1}}\n",
    "\\ba\n",
    " p &= \\frac{TP}{TP + FP}\n",
    "\\ea\n",
    "$$\n",
    "\n",
    "### Recall or Sensitivity\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    r &= \\frac{TP}{TP + FN}\\\\\n",
    "      &= \\frac{TP}{TP + FN}\\\\ \n",
    "      &= \\frac{TP}{N_P}\n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "      a &= \\frac{TP + TN}{TP + TN + FP + FN}\\\\\n",
    "        &= \\frac{TP + TN}{N}\\\\\n",
    "        &= \\frac{TP + TN}{N_P + N_N} \n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "#### Accuracy Paradox\n",
    "\n",
    "A static constant model, say $P\\st{cancer}=0$ may have a higher accuracy than a real model with predictive power. \n",
    "\n",
    "For the accuracy-metric the cost of false positives are taken to be equal to\n",
    "true negatives.\n",
    "\n",
    "### F-score\n",
    "\n",
    "General $\\beta$-harmonic mean of the precision and recall \n",
    "$$\n",
    "    F_\\beta = (1+\\beta^2) \\frac{2pr}{\\beta^2 p+r}\\\\\n",
    "$$ \n",
    "that for say $\\beta=2$ or $\\beta=0.5$ shifts or skews the emphasis on the two variables in the equation. Normally only the $\\beta=1$ harmonic mean is used\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    F_1     &= \\frac{2pr}{p+r}\\\\\n",
    "            &=\\frac{2}{1/p + 1/r}\n",
    "  \\ea\n",
    "$$\n",
    "with $F$ typically being synonymous with $F_1$.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "For statistical classification the confusion matrix, or error matrix (or\n",
    "matching matrix in unsupervised learning) is for a two-class problem given by\n",
    "the $2\\times2$ matrix with dimensions 'actual' and 'predicted'\n",
    "\n",
    "$$   \n",
    "{\\bf M}\\st{confusion} = \n",
    "\\begin{array}{l|ll}\n",
    "                           & \\mbox{actual true} & \\mbox{actual false} \\\\ \\hline\n",
    "    \\mbox{predicted true}  & TP & FP \\\\     \n",
    "    \\mbox{predicted false} & FN & TN \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The diagonal, in the square matrix, represent predicted values being the same\n",
    "as the actual values, off-diagonal elements represents erroneous prediction.\n",
    "\n",
    "For N-class classification the matrix gives a matrix with $N$ actual\n",
    "classes and $N$ predicted classes\n",
    "\n",
    "$$\n",
    "{\\bf M}\\st{confusion} =\n",
    "  \\left[\n",
    "  \\begin{array}{llll}\n",
    "       c_{11} & c_{12} & \\cdots & c_{1n} \\\\ \n",
    "       c_{21} & c_{22} & \\cdots & c_{2n} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       c_{n1} & c_{n2} & \\cdots & c_{nn} \\\\ \n",
    "  \\end{array}\n",
    "  \\right]\n",
    "$$\n",
    "with say element $c_{21}$ being the number of actual classses '1' being predited (erroneously) as class '2'.\n",
    "\n",
    "### Nomenclature for the Confusion Matrix\n",
    "\n",
    "![performance metrics](performance_metrics.png)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "#### Qa Implement an accuracy function and test it on the MNIST data.\n",
    "\n",
    "The function `TestAccuracy` will checks your implementaion against the build-in `sklearn.metrics.accuracy_score` and the two functions should yield the same result. If they do not the test-vector function will let you know, so all you have to do is to implement a `MyAccuracy` function with the right parameter signature.\n",
    "\n",
    "Test it both on your dummy classifier and on the Stochastic Gradient Descent classifier (with setup parameters as in [HOLM])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "def MyPrecision(clf, y_pred, y_true):\n",
    "    # TODO: you impl here\n",
    "\n",
    "\n",
    "# TEST FUNCTION: compare with Scikit-learn accuracy_score\n",
    "def TestAccuracy(clf, y_pred, y_true):\n",
    "    a0=MyAccuracy(clf,y_pred, y_true)\n",
    "    a1=accuracy_score(y_pred, y_true)\n",
    "\n",
    "    print(\"\\nmy a          =\",a0)\n",
    "    print(\"scikit-learn a=\",a1)\n",
    "\n",
    "    itmalutils.InRange(a0,a1)\n",
    "\n",
    "# TEST VECTORS:\n",
    "TestAccuracy(dmy, dmy_y_train_pred, )\n",
    "TestAccuracy(sgd, sgd_y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb Implement a F$_1$ function and test it on the MNIST data.\n",
    "\n",
    "Test it both on your dummy classifier and on the Stochastic Gradient Descent classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def MyPrecision(clf,X,y):\n",
    "    \n",
    "\n",
    "def MyRecall(clf,X,y):\n",
    "    \n",
    "def MyF1(clf,X,y):\n",
    "    \n",
    "def TestMetrics(clf,y_train_pred):\n",
    "    p =MyPrecision(clf,X_train,y_train_5)\n",
    "    pt=precision_score(y_train_5,y_train_pred)\n",
    "    \n",
    "    r =MyRecall(clf,X_train,y_train_5)\n",
    "    rt=recall_score(y_train_5,y_train_pred)\n",
    "    \n",
    "    f =MyF1(clf,X_train,y_train_5)\n",
    "    ft=f1_score(y_train_5,y_train_pred)\n",
    "\n",
    "    print(\"p =\",p)\n",
    "    print(\"pt=\",pt)\n",
    "    print(\"r =\",r)\n",
    "    print(\"rt=\",rt)\n",
    "    print(\"f =\",f)\n",
    "    print(\"ft=\",ft)\n",
    "\n",
    "    itmalutils.InRange(p,pt)\n",
    "    itmalutils.InRange(r,rt)\n",
    "    itmalutils.InRange(f,ft)\n",
    "\n",
    "#TestMetrics(dmy,dmy_y_train_pred)\n",
    "TestMetrics(sgd,sgd_y_train_pred)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q try to yield a confusion matrix output for the non-stratified test data via  `sklearn.metrics.confusion_matrix` function.\n",
    "\n",
    "How are the Scikit-learn confusion matrix organized, where are the TP, FP, FN and TN located in the matrix (indicies), and what happens if you mess up the parameters calling\n",
    "\n",
    "```python\n",
    "confusion_matrix(y_train_pred,y_train_5)\n",
    "```\n",
    "\n",
    "instead of \n",
    "```python\n",
    "confusion_matrix(y_train_5,y_train_pred)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
